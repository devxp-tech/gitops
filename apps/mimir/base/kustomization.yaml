apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
namespace: monitoring
helmCharts:
  - name: mimir-distributed
    includeCRDs: true
    releaseName: mimir-distributed
    namespace: monitoring
    version: "5.4.0"
    repo: https://grafana.github.io/helm-charts
    valuesInline:
      fullnameOverride: mimir
      global:
        extraEnv:
          - name: JAEGER_AGENT_HOST
            value: otel-collector.observability.svc.cluster.local
          - name: JAEGER_AGENT_PORT
            value: "6831"
          - name: JAEGER_SAMPLER_TYPE
            value: const
          - name: JAEGER_SAMPLER_PARAM
            value: "1"
      serviceAccount:
        create: true
        name: mimir
        annotations:
          eks.amazonaws.com/role-arn: "arn:aws:iam::239468932737:role/mimir-lgseksd1-sa"

      mimir:
        structuredConfig:
          memberlist:
            cluster_label: "mimir-monitoring"
          common:
            storage:
              backend: s3
              s3:
                bucket_name: vex-dev-mimir-metrics
                endpoint: s3.us-east-1.amazonaws.com
          blocks_storage:
            storage_prefix: blocks
            s3:
              bucket_name: vex-dev-mimir-metrics
              insecure: true
          usage_stats:
            enabled: false
            installation_mode: helm
          distributor:
            remote_timeout: 4s
          compactor:
            data_dir: /data
          multitenancy_enabled: false
          frontend:
            align_queries_with_step: true
            log_queries_longer_than: 10s
          ingester_client:
            grpc_client_config:
              max_recv_msg_size: 104857600
              max_send_msg_size: 104857600
          server:
            log_level: info
            grpc_server_max_concurrent_streams: 1000
            grpc_server_max_recv_msg_size: 104857600
            grpc_server_max_send_msg_size: 104857600
          limits:
            compactor_blocks_retention_period: 15d
            ingestion_rate: 80000
            max_global_series_per_metric: 0
            max_global_series_per_user: 0
            max_label_names_per_series: 50
            ruler_max_rules_per_rule_group: 80
          ruler:
            alertmanager_url: http://prometheus-community-kube-alertmanager.monitoring.svc.cluster.local:9093
          runtime_config:
            file: /var/mimir/runtime.yaml

      alertmanager:
        enabled: false

      compactor:
        persistentVolume:
          size: 20Gi
        resources:
          limits:
            cpu: 200m
            memory: 396Mi # 30% of 2.1Gi
          requests:
            cpu: 50m   # 30% of 1
            memory: 128Mi # 30% of 1.5Gi
        tolerations:
          - key: "role"
            operator: "Equal"
            value: "on-demand"
            effect: "NoSchedule"
        nodeSelector:
          role: "on-demand"

      distributor:
        replicas: 2
        podDisruptionBudget:
          maxUnavailable: 2
        resources:
          limits:
            memory: 1536Mi # 30% of 5.7Gi
          requests:
            cpu: 400m   # 30% of 2
            memory: 1024Mi # 30% of 4Gi
        tolerations:
          - key: "role"
            operator: "Equal"
            value: "on-demand"
            effect: "NoSchedule"
        nodeSelector:
          role: "on-demand"

      ingester:
        persistentVolume:
          size: 10Gi
        resources:
          limits:
            memory: 2.8Gi # 30% of 12Gi
          requests:
            cpu: 500m   # 30% of 3.5
            memory: 1.8Gi # 30% of 8Gi
        replicas: 3
        podDisruptionBudget:
          maxUnavailable: 2
        zoneAwareReplication:
          enabled: false
        tolerations:
          - key: "role"
            operator: "Equal"
            value: "on-demand"
            effect: "NoSchedule"
        nodeSelector:
          role: "on-demand"

      admin-cache:
        enabled: true
        replicas: 1 #2

      chunks-cache:
        enabled: true
        replicas: 1 #2
        allocatedMemory: 128
        resources:
          requests:
            cpu: 30m

      index-cache:
        enabled: true
        replicas: 1 #3
        allocatedMemory: 64
        resources:
          requests:
            cpu: 30m

      metadata-cache:
        enabled: true
        allocatedMemory: 64
        resources:
          requests:
            cpu: 10m

      results-cache:
        enabled: true
        replicas: 1 #2
        allocatedMemory: 64
        resources:
          requests:
            cpu: 30m

      minio:
        enabled: false

      overrides_exporter:
        replicas: 1
        resources:
          limits:
            memory: 38Mi # 30% of 128Mi
          requests:
            cpu: 30m   # 30% of 100m
            memory: 38Mi # 30% of 128Mi

      querier:
        replicas: 1
        resources:
          limits:
            memory: 256Mi # 30% of 5.6Gi
          requests:
            cpu: 100m   # 30% of 2
            memory: 128Mi # 30% of 4Gi
        tolerations:
          - key: "role"
            operator: "Equal"
            value: "on-demand"
            effect: "NoSchedule"
        nodeSelector:
          role: "on-demand"

      query_frontend:
        replicas: 1
        resources:
          limits:
            memory: 384Mi # 30% of 2.8Gi
          requests:
            cpu: 100m   # 30% of 2
            memory: 256Mi # 30% of 2Gi
        tolerations:
          - key: "role"
            operator: "Equal"
            value: "on-demand"
            effect: "NoSchedule"
        nodeSelector:
          role: "on-demand"

      ruler:
        replicas: 1
        resources:
          limits:
            memory: 2028Mi # 50% of 2.8Gi
          requests:
            cpu: 500m   # 50% of 1
            memory: 1536Mi # 50% of 2Gi
        # tolerations:
        #   - key: "role"
        #     operator: "Equal"
        #     value: "on-demand"
        #     effect: "NoSchedule"
        # nodeSelector:
        #   role: "on-demand"

      store_gateway:
        persistentVolume:
          size: 10Gi
        resources:
          limits:
            memory: 640Mi # 30% of 2.1Gi
          requests:
            cpu: 100m   # 30% of 1
            memory: 256Mi # 30% of 1.5Gi
        replicas: 3
        podDisruptionBudget:
          maxUnavailable: 2
        zoneAwareReplication:
          enabled: false
        tolerations:
          - key: "role"
            operator: "Equal"
            value: "on-demand"
            effect: "NoSchedule"
        nodeSelector:
          role: "on-demand"

      nginx:
        replicas: 1
        resources:
          requests:
            cpu: 100m   # 30% of 1
            memory: 64Mi # 30% of 512Mi
          limits:
            memory: 128Mi # 30% of 731Mi
            cpu: 100m   # 30% of 1

      gateway:
        replicas: 1
        resources:
          limits:
            memory: 219Mi # 30% of 731Mi
          requests:
            cpu: 100m   # 30% of 1
            memory: 154Mi # 30% of 512Mi

      metaMonitoring:
        dashboards:
          enabled: true
          annotations:
            k8s-sidecar-target-directory: "/var/lib/grafana/dashboards/mimir"
        serviceMonitor:
          enabled: true
        prometheusRule:
          enabled: true
          mimirAlerts: true
          mimirRules: true
