apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
namespace: monitoring
helmCharts:
  - name: mimir-distributed
    includeCRDs: true
    releaseName: mimir-distributed
    namespace: monitoring
    version: "5.4.0"
    repo: https://grafana.github.io/helm-charts
    valuesInline:
      fullnameOverride: mimir
      global:
        extraEnv:
          - name: JAEGER_AGENT_HOST
            value: otel-collector.observability.svc.cluster.local
          - name: JAEGER_AGENT_PORT
            value: "6831"
          - name: JAEGER_SAMPLER_TYPE
            value: const
          - name: JAEGER_SAMPLER_PARAM
            value: "1"
      serviceAccount:
        create: true
        name: mimir
        annotations:
          eks.amazonaws.com/role-arn: "arn:aws:iam::239468932737:role/mimir-lgseksd1-sa"
      mimir:
        structuredConfig:
          common:
            storage:
              backend: s3
              s3:
                bucket_name: vex-dev-mimir-metrics
                endpoint: s3.us-east-1.amazonaws.com
          blocks_storage:
            storage_prefix: blocks
            s3:
              bucket_name: vex-dev-mimir-metrics
              insecure: true
          usage_stats:
            enabled: false
            installation_mode: helm
          distributor:
            remote_timeout: 4s
          compactor:
            data_dir: /data
          multitenancy_enabled: false
          frontend:
            align_queries_with_step: true
            log_queries_longer_than: 10s
          ingester_client:
            grpc_client_config:
              max_recv_msg_size: 104857600
              max_send_msg_size: 104857600
          server:
            log_level: info
            grpc_server_max_concurrent_streams: 1000
            grpc_server_max_recv_msg_size: 104857600
            grpc_server_max_send_msg_size: 104857600
          limits:
            compactor_blocks_retention_period: 15d
            ingestion_rate: 80000
            max_global_series_per_metric: 0
            max_global_series_per_user: 0
            max_label_names_per_series: 40
            ruler_max_rules_per_rule_group: 80
          ruler:
            alertmanager_url: http://prometheus-community-kube-alertmanager.monitoring.svc.cluster.local:9093
          runtime_config:
            file: /var/mimir/runtime.yaml
      alertmanager:
        enabled: false
      # minio:
      #   enabled: false
      # compactor:
      #   persistentVolume:
      #     size: 20Gi
      # distributor:
      #   replicas: 1
      #   tolerations:
      #     - key: "role"
      #       operator: "Equal"
      #       value: "o11y"
      #       effect: "NoSchedule"
      #   nodeSelector:
      #     role: "o11y"
      #   extraArgs:
      #     distributor.ingestion-rate-limit: "1000000"
      #     distributor.ingestion-burst-size: "10000000"
      #     validation.max-label-names-per-series: "50"
      # ingester:
      #   replicas: 3
      #   podDisruptionBudget:
      #     maxUnavailable: 2
      #   tolerations:
      #     - key: "role"
      #       operator: "Equal"
      #       value: "o11y"
      #       effect: "NoSchedule"
      #   nodeSelector:
      #     role: "o11y"
      #   persistentVolume:
      #     size: 8Gi
      #   zoneAwareReplication:
      #     enabled: false
      # nginx:
      #   replicas: 2
      #   tolerations:
      #     - key: "role"
      #       operator: "Equal"
      #       value: "spot"
      #       effect: "NoSchedule"
      #   nodeSelector:
      #     role: "spot"
      #   podDisruptionBudget:
      #     maxUnavailable: 2
      # querier:
      #   replicas: 1
      #   tolerations:
      #     - key: "role"
      #       operator: "Equal"
      #       value: "o11y"
      #       effect: "NoSchedule"
      #   nodeSelector:
      #     role: "o11y"
      #   podDisruptionBudget:
      #     maxUnavailable: 1
      # ruler:
      #   enabled: true
      #   replicas: 1
      #   tolerations:
      #     - key: "role"
      #       operator: "Equal"
      #       value: "on-demand"
      #       effect: "NoSchedule"
      #   nodeSelector:
      #     role: "on-demand"
      #   podDisruptionBudget:
      #     maxUnavailable: 1
      # query_frontend:
      #   replicas: 1
      #   tolerations:
      #     - key: "role"
      #       operator: "Equal"
      #       value: "o11y"
      #       effect: "NoSchedule"
      #   nodeSelector:
      #     role: "o11y"
      #   podDisruptionBudget:
      #     maxUnavailable: 1
      # query_scheduler:
      #   replicas: 1
      #   podDisruptionBudget:
      #     maxUnavailable: 1
      #   tolerations:
      #     - key: "role"
      #       operator: "Equal"
      #       value: "o11y"
      #       effect: "NoSchedule"
      #   nodeSelector:
      #     role: "o11y"
      # store_gateway:
      #   replicas: 2
      #   podDisruptionBudget:
      #     maxUnavailable: 2
      #   tolerations:
      #     - key: "role"
      #       operator: "Equal"
      #       value: "o11y"
      #       effect: "NoSchedule"
      #   nodeSelector:
      #     role: "o11y"
      #   zoneAwareReplication:
      #     enabled: false


      compactor:
        persistentVolume:
          size: 20Gi
        resources:
          limits:
            memory: 2.1Gi
          requests:
            cpu: 1
            memory: 1.5Gi
        tolerations:
          - key: "role"
            operator: "Equal"
            value: "o11y"
            effect: "NoSchedule"
        nodeSelector:
          role: "o11y"

      distributor:
        replicas: 2
        resources:
          limits:
            memory: 5.7Gi
          requests:
            cpu: 2
            memory: 4Gi
        tolerations:
          - key: "role"
            operator: "Equal"
            value: "o11y"
            effect: "NoSchedule"
        nodeSelector:
          role: "o11y"

      ingester:
        persistentVolume:
          size: 50Gi
        replicas: 3
        resources:
          limits:
            memory: 12Gi
          requests:
            cpu: 3.5
            memory: 8Gi
        topologySpreadConstraints: {}
        affinity:
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              - labelSelector:
                  matchExpressions:
                    - key: target # support for enterprise.legacyLabels
                      operator: In
                      values:
                        - ingester
                topologyKey: 'kubernetes.io/hostname'

              - labelSelector:
                  matchExpressions:
                    - key: app.kubernetes.io/component
                      operator: In
                      values:
                        - ingester
                topologyKey: 'kubernetes.io/hostname'

        zoneAwareReplication:
          topologyKey: 'kubernetes.io/hostname'
        tolerations:
          - key: "role"
            operator: "Equal"
            value: "o11y"
            effect: "NoSchedule"
        nodeSelector:
          role: "o11y"

      admin-cache:
        enabled: true
        replicas: 2

      chunks-cache:
        enabled: true
        replicas: 2

      index-cache:
        enabled: true
        replicas: 3

      metadata-cache:
        enabled: true

      results-cache:
        enabled: true
        replicas: 2

      minio:
        enabled: false

      overrides_exporter:
        replicas: 1
        resources:
          limits:
            memory: 128Mi
          requests:
            cpu: 100m
            memory: 128Mi

      querier:
        replicas: 1
        resources:
          limits:
            memory: 5.6Gi
          requests:
            cpu: 2
            memory: 4Gi
        tolerations:
          - key: "role"
            operator: "Equal"
            value: "o11y"
            effect: "NoSchedule"
        nodeSelector:
          role: "o11y"

      query_frontend:
        replicas: 1
        resources:
          limits:
            memory: 2.8Gi
          requests:
            cpu: 2
            memory: 2Gi
        tolerations:
          - key: "role"
            operator: "Equal"
            value: "o11y"
            effect: "NoSchedule"
        nodeSelector:
          role: "o11y"

      ruler:
        replicas: 1
        resources:
          limits:
            memory: 2.8Gi
          requests:
            cpu: 1
            memory: 2Gi
        tolerations:
          - key: "role"
            operator: "Equal"
            value: "o11y"
            effect: "NoSchedule"
        nodeSelector:
          role: "o11y"

      store_gateway:
        persistentVolume:
          size: 10Gi
        replicas: 3
        resources:
          limits:
            memory: 2.1Gi
          requests:
            cpu: 1
            memory: 1.5Gi
        topologySpreadConstraints: {}
        affinity:
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              - labelSelector:
                  matchExpressions:
                    - key: target # support for enterprise.legacyLabels
                      operator: In
                      values:
                        - store-gateway
                topologyKey: 'kubernetes.io/hostname'

              - labelSelector:
                  matchExpressions:
                    - key: app.kubernetes.io/component
                      operator: In
                      values:
                        - store-gateway
                topologyKey: 'kubernetes.io/hostname'
        zoneAwareReplication:
          topologyKey: 'kubernetes.io/hostname'
        tolerations:
          - key: "role"
            operator: "Equal"
            value: "o11y"
            effect: "NoSchedule"
        nodeSelector:
          role: "o11y"

      nginx:
        replicas: 1
        resources:
          limits:
            memory: 731Mi
          requests:
            cpu: 1
            memory: 512Mi

      gateway:
        replicas: 1
        resources:
          limits:
            memory: 731Mi
          requests:
            cpu: 1
            memory: 512Mi

      metaMonitoring:
        dashboards:
          enabled: true
          annotations:
            k8s-sidecar-target-directory: "/var/lib/grafana/dashboards/mimir"
        serviceMonitor:
          enabled: true
        prometheusRule:
          enabled: true
          mimirAlerts: true
          mimirRules: true
